{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "from hotelling.stats import hotelling_t2\n",
    "import statsmodels.api as sm \n",
    "from sklearn.model_selection import train_test_split \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using 18 components for 99% variance\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "df = pd.read_csv('../figures/out/gdd_all_reduced_forR.csv',index_col=0)\n",
    "\n",
    "# PCA\n",
    "features = [f'G{i}' for i in range(30)]\n",
    "x = df.loc[:, features].values\n",
    "y = df.loc[:,['chol']].values\n",
    "x = StandardScaler().fit_transform(x)\n",
    "pca = PCA()\n",
    "principalComponents = pca.fit_transform(x)\n",
    "evr = pca.explained_variance_ratio_.cumsum()\n",
    "for i,j in enumerate(evr):\n",
    "    if j > 0.99:\n",
    "        nPCs = i + 1\n",
    "        break\n",
    "print(f'using {nPCs} components for 99% variance')\n",
    "pca = PCA(n_components=nPCs)\n",
    "principalComponents = pca.fit_transform(x)\n",
    "principalDf = pd.DataFrame(data = principalComponents\n",
    "            , columns = [f'PC{x}' for x in range(1,nPCs+1)])\n",
    "finalDf = pd.concat([principalDf, df[['chol','replicate']]], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hotelling P = 1.6854592283467506e-224\n"
     ]
    }
   ],
   "source": [
    "# Hotelling p https://dionresearch.github.io/hotelling/modules.html#module-hotelling.stats\n",
    "# have to use PCs instead of raw features I think because some features are perfectly correlated?\n",
    "    # If I don't use PCs it returns an error\n",
    "x = finalDf[finalDf['chol'] == 15].drop(columns = ['chol','replicate']).to_numpy()\n",
    "y = finalDf[finalDf['chol'] == 30].drop(columns = ['chol','replicate']).to_numpy()\n",
    "print(f'hotelling P = {hotelling_t2(x,y)[2]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.443841\n",
      "         Iterations 9\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                   chol   No. Observations:                 1920\n",
      "Model:                          Logit   Df Residuals:                     1902\n",
      "Method:                           MLE   Df Model:                           17\n",
      "Date:                Thu, 18 Apr 2024   Pseudo R-squ.:                  0.3578\n",
      "Time:                        18:46:54   Log-Likelihood:                -852.17\n",
      "converged:                       True   LL-Null:                       -1327.0\n",
      "Covariance Type:            nonrobust   LLR p-value:                5.432e-191\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "x1            -0.6078      0.072     -8.460      0.000      -0.749      -0.467\n",
      "x2            -2.4429      0.358     -6.828      0.000      -3.144      -1.742\n",
      "x3            -0.4127      0.161     -2.557      0.011      -0.729      -0.096\n",
      "x4            -0.5544      0.071     -7.806      0.000      -0.694      -0.415\n",
      "x5             0.2139      0.177      1.206      0.228      -0.134       0.561\n",
      "x6            -0.8230      0.247     -3.329      0.001      -1.308      -0.338\n",
      "x7           -13.2048      2.588     -5.102      0.000     -18.278      -8.132\n",
      "x8            -1.6025      0.222     -7.204      0.000      -2.038      -1.167\n",
      "x9            -2.3264      0.327     -7.120      0.000      -2.967      -1.686\n",
      "x10           -1.4979      0.307     -4.873      0.000      -2.100      -0.895\n",
      "x11           -1.1163      0.239     -4.673      0.000      -1.585      -0.648\n",
      "x12           -0.8723      0.099     -8.799      0.000      -1.067      -0.678\n",
      "x13            0.2093      0.063      3.311      0.001       0.085       0.333\n",
      "x14           -0.3064      0.063     -4.882      0.000      -0.429      -0.183\n",
      "x15            0.2376      0.066      3.574      0.000       0.107       0.368\n",
      "x16           -0.8496      0.067    -12.677      0.000      -0.981      -0.718\n",
      "x17           -0.0402      0.061     -0.664      0.507      -0.159       0.078\n",
      "x18           -0.0543      0.062     -0.875      0.381      -0.176       0.067\n",
      "==============================================================================\n",
      "prediction accuracy: 0.7645833333333333\n"
     ]
    }
   ],
   "source": [
    "# Logistic model   \n",
    "X = finalDf.drop(['chol','replicate'],axis=1)\n",
    "y = (finalDf['chol'] - 15 )/15\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y , \n",
    "                                random_state=104,  \n",
    "                                train_size=0.8,  \n",
    "                                shuffle=True) \n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "log_reg = sm.Logit(y_train, X_train).fit() \n",
    "\n",
    "# see LLR p-value for p value\n",
    "print(log_reg.summary())\n",
    "\n",
    "\n",
    "yhat = log_reg.predict(X_test) \n",
    "prediction = list(map(round, yhat)) \n",
    "print(f'prediction accuracy: {np.sum(y_test == prediction)/len(y_test)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
